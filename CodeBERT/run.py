"""
@author: xuty
@contact:xuty1218@163.com
@version: 1.0.0
@file: run.py
@time: 2025/5/13 17:48
"""
import os
import torch
import numpy as np
from tqdm import tqdm
import json
import logging
from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset
import torch.nn as nn
logger = logging.getLogger(__name__)


class CodeBERTInputFeatures(object):  # ç”¨äºå­˜å‚¨å•ä¸ªè®­ç»ƒæˆ–æµ‹è¯•æ ·æœ¬çš„ç‰¹å¾ï¼Œè¿™ä¸ªç±»çš„è®¾è®¡ä¸»è¦æ˜¯ä¸ºäº†æ–¹ä¾¿ç»„ç»‡å’Œè®¿é—®æ¯ä¸ªæ ·æœ¬çš„ä¸åŒç‰¹å¾
    # å•ä¸ªè®­ç»ƒ/æµ‹è¯•æ ·æœ¬çš„ç‰¹å¾ (A single training/test features for a example)
    def __init__(self, input_tokens, input_ids, idx, label):
        self.input_tokens = input_tokens  # ç»è¿‡åˆ†è¯å™¨å¤„ç†åçš„è¾“å…¥ä»£ç çš„tokensåºåˆ—
        self.input_ids = input_ids  # å°†tokenè½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ•°å­—IDåºåˆ—
        self.idx=idx  # æ ·æœ¬çš„å”¯ä¸€æ ‡è¯†ç¬¦
        self.label=label  # æ ·æœ¬çš„æ ‡ç­¾

def codebert_convert_examples_to_features(js,tokenizer,args):  # è´Ÿè´£å°†åŸå§‹çš„JSONæ ¼å¼æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„InputFeatureså¯¹è±¡
    #source
    # 1ã€å¤„ç†æºä»£ç 
    code=' '.join(js['func'].split())  # å»é™¤è¾“å…¥ä»£ç å¤šä½™çš„ç©ºæ ¼ï¼Œåªä¿ç•™å•è¯ä¹‹é—´çš„å•ä¸ªç©ºæ ¼  .split()+' '.join(...)
    code_tokens=tokenizer.tokenize(code)[:args.block_size-2]  # ä½¿ç”¨tokenizerå°†ä»£ç æ–‡æœ¬åˆ†å‰²æˆtokenï¼Œå¹¶å¯¹åˆ†è¯ç»“æœæˆªæ–­ï¼Œ[:args.block_size-2]è¡¨ç¤ºä»åˆ†è¯ç»“æœä¸­æå–å‰args.block_size-2ä¸ªtoken
    # 2ã€æ·»åŠ ç‰¹æ®Štoken
    source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]  # åœ¨ä»£ç tokenåºåˆ—çš„å¼€å¤´æ·»åŠ [CLS]ï¼Œæœ«å°¾æ·»åŠ [SEP]  æ˜¯ä¹‹å‰å·²ç»åˆ†è¯å¹¶æˆªæ–­åçš„ä»£ç tokenåˆ—è¡¨
    # 3ã€è½¬æ¢ä¸ºID
    source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)  # å°†tokenåºåˆ—è½¬æ¢ä¸ºå¯¹åº”çš„IDåˆ—è¡¨  æ¯ä¸ªtokenåœ¨åˆ†è¯å™¨çš„è¯æ±‡è¡¨ä¸­éƒ½æœ‰ä¸€ä¸ªå”¯ä¸€çš„ID
    # 4ã€å¡«å……ï¼Œç¡®ä¿æ¯ä¸ªè¾“å…¥åºåˆ—çš„é•¿åº¦ä¸€è‡´ï¼Œä¾¿äºæ¨¡å‹è¿›è¡Œæ‰¹é‡å¤„ç†
    padding_length = args.block_size - len(source_ids)  # è®¡ç®—éœ€è¦å¡«å……çš„é•¿åº¦ï¼Œä½¿å¾—å½“å‰çš„source_idsé•¿åº¦è¾¾åˆ°args.block_size  args.block_sizeæ˜¯æˆªæ–­é˜¶æ®µæåˆ°çš„å—å¤§å°å‚æ•°ï¼Œè¡¨ç¤ºæ¯ä¸ªè¾“å…¥åºåˆ—çš„æœ€å¤§é•¿åº¦
    source_ids+=[tokenizer.pad_token_id]*padding_length  # ä½¿ç”¨å¡«å……tokençš„IDè¿›è¡Œå¡«å……ï¼Œtokenizer.pad_token_idæ˜¯åˆ†è¯å™¨ä¸­çš„ä¸€ä¸ªç‰¹æ®Štokençš„IDï¼Œé€šå¸¸ç”¨äºå¡«å……åºåˆ—ï¼Œä½¿å…¶è¾¾åˆ°å›ºå®šé•¿åº¦ï¼Œå³å°†source_idsåˆ—è¡¨æ‰©å±•åˆ°args.block_sizeçš„é•¿åº¦

    return CodeBERTInputFeatures(source_tokens, source_ids, js['idx'], int(js['target']))


class CodeBERTTextDataset(Dataset):  # åŠ è½½å’Œé¢„å¤„ç†ä»£ç æ¼æ´æ£€æµ‹æ•°æ®é›†
    def __init__(self, tokenizer, args, file_path=None):  # æ¥æ”¶tokenizerã€å‚æ•°å’Œæ–‡ä»¶è·¯å¾„ä½œä¸ºè¾“å…¥
        self.examples = []  # selfç›¸å½“äºå…¨å±€å˜é‡ï¼Œå› ä¸ºå‡½æ•°å†…éƒ¨å˜é‡æ— æ³•ä¼ é€’ä½†selfå¯ä»¥ æ„é€ å‡½æ•°é‡Œçš„self.examplesåœ¨åé¢çš„å‡½æ•°é‡Œéƒ½å¯ä»¥ç”¨
        # åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨self.examplesæ¥å­˜å‚¨æ‰€æœ‰æ ·æœ¬

        file_type = file_path.split('/')[-1].split('.')[0]  # åˆ†å‰²file_pathå–æœ€åä¸€ä¸ªå…ƒç´ ï¼ŒæŒ‰.åˆ†å‰²åå–ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œå³æ‰¾åˆ°è¾“å…¥çš„æ•°æ®é›†çš„ç±»å‹ï¼Œæ˜¯testè¿˜æ˜¯train?
        folder = '/'.join(file_path.split('/')[:-1])  # å»æ‰file_pathæœ€åä¸€ä¸ª'/xxx'
        cache_file_path = os.path.join(folder, 'codebert_cached_{}'.format(file_type))

        try:
            self.examples = torch.load(cache_file_path)
        except:
            with open(file_path) as f:
                for line in f:  # æ‰“å¼€æŒ‡å®šæ–‡ä»¶ï¼Œé€è¡Œè¯»å–å¹¶å¤„ç†
                    js = json.loads(line.strip())  # æ¯è¡Œæ˜¯ä¸€ä¸ªJSONæ ¼å¼çš„å­—ç¬¦ä¸²ï¼Œä½¿ç”¨json.loadsè§£æä¸ºå­—å…¸js
                    self.examples.append(codebert_convert_examples_to_features(js, tokenizer, args))  # è°ƒç”¨convert_examples_to_featureså‡½æ•°å°†å­—å…¸jsè½¬æ¢ä¸ºInputFeatureså¯¹è±¡
            torch.save(self.examples, cache_file_path)

        # with open(file_path) as f:
        #     for line in f:  # æ‰“å¼€æŒ‡å®šæ–‡ä»¶ï¼Œé€è¡Œè¯»å–å¹¶å¤„ç†
        #         js=json.loads(line.strip())  # æ¯è¡Œæ˜¯ä¸€ä¸ªJSONæ ¼å¼çš„å­—ç¬¦ä¸²ï¼Œä½¿ç”¨json.loadsè§£æä¸ºå­—å…¸js
        #         self.examples.append(codebert_convert_examples_to_features(js, tokenizer, args))  # è°ƒç”¨convert_examples_to_featureså‡½æ•°å°†å­—å…¸jsè½¬æ¢ä¸ºInputFeatureså¯¹è±¡
        #         # è½¬æ¢åå°†ç»“æœå­˜å‚¨åœ¨ self.examples åˆ—è¡¨ä¸­

        if 'train' == file_type:  # å¦‚æœæ˜¯è®­ç»ƒæ–‡ä»¶ï¼ˆæ–‡ä»¶è·¯å¾„åŒ…å«'train'ï¼‰ ä¸»è¦ç›®çš„æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ‰“å°ä¸€äº›æ ·æœ¬ä¿¡æ¯ç”¨äºè°ƒè¯•å’ŒéªŒè¯
            for idx, example in enumerate(self.examples[:3]):  # æ‰“å°å‰3ä¸ªæ ·æœ¬çš„ä¿¡æ¯ï¼ˆåŒ…æ‹¬ç´¢å¼•ã€æ ‡ç­¾ã€è¾“å…¥tokenå’Œè¾“å…¥IDï¼‰ç”¨äºè°ƒè¯•
                    logger.info("*** Example ***")
                    logger.info("idx: {}".format(idx))
                    logger.info("label: {}".format(example.label))
                    logger.info("input_tokens: {}".format([x.replace('\u0120','_') for x in example.input_tokens]))
                    # å°†input_tokensä¸­çš„æ¯ä¸ªtokenä¸­çš„ç‰¹æ®Šå­—ç¬¦\u0120ï¼ˆå³ç©ºæ ¼ï¼‰æ›¿æ¢ä¸º_ï¼Œç„¶åå°†å¤„ç†åçš„tokenåˆ—è¡¨æ‰“å°åˆ°æ—¥å¿—ä¸­
                    logger.info("input_ids: {}".format(' '.join(map(str, example.input_ids))))
                    # mapå‡½æ•°å°†example.input_idsä¸­çš„æ¯ä¸ªIDè½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œ' '.join()å°†å­—ç¬¦ä¸²åˆ—è¡¨ç”¨ç©ºæ ¼è¿æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²

    def __len__(self):  # è¿”å›æ•°æ®é›†ä¸­æ ·æœ¬çš„æ•°é‡  å¯ä»¥ç›´æ¥len(å®ä¾‹åŒ–çš„å¯¹è±¡)è¿”å›æ•°æ®é›†é•¿åº¦
        return len(self.examples)

    def __getitem__(self, i):  # æ”¹å†™å…³äº"ç´¢å¼•"çš„å†…ç½®æ–¹æ³•ï¼ŒTextDataset[idx] ä¼šè¿”å›è¯¥æ–¹æ³•çš„returnçš„å†…å®¹
        # æ ¹æ®ç´¢å¼•iè·å–æ•°æ®é›†ä¸­çš„ç¬¬iä¸ªæ ·æœ¬ï¼Œè¿”å›çš„æ˜¯ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«ä¸‰ä¸ªå¼ é‡ï¼šè¾“å…¥IDã€æ ‡ç­¾å’Œç´¢å¼•
        return torch.tensor(self.examples[i].input_ids), torch.tensor(self.examples[i].label), torch.tensor(self.examples[i].idx)  # å°†æ ·æœ¬çš„input_idsã€labelå’Œidxè½¬æ¢ä¸ºPyTorchå¼ é‡


def evaluate(args, model, tokenizer):
    """è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼Œå¹¶è®¡ç®—æ¢¯åº¦å½’å› å’ŒL2èŒƒæ•°"""
    eval_dataset = CodeBERTTextDataset(tokenizer, args, args.test_data_file)
    eval_sampler = SequentialSampler(eval_dataset)
    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)

    # è·å–åµŒå…¥å±‚
    active_path = 'encoder.roberta.embeddings.word_embeddings'
    try:
        parts = active_path.split('.')
        embedding_layer = model
        for part in parts:
            embedding_layer = getattr(embedding_layer, part)
        print(f"ä½¿ç”¨åµŒå…¥å±‚è·¯å¾„: {active_path}")
    except AttributeError:
        raise ValueError(f"æ— æ³•æ‰¾åˆ°åµŒå…¥å±‚è·¯å¾„ {active_path}ï¼Œè¯·æ£€æŸ¥æ¨¡å‹ç»“æ„")

    # è¯„ä¼°
    model.eval()
    logits = []
    labels = []
    tokens_by_idx_to_save = {}
    grads_by_idx_to_save = {}
    tokens_by_idx_for_lookup = {example.idx: example.input_tokens for example in eval_dataset.examples}

    for batch in tqdm(eval_dataloader, desc="Evaluating and computing attributions"):
        input_ids = batch[0].to(args.device)
        label = batch[1].to(args.device)
        input_ids.requires_grad_(False)
        label1 = label
        label_for_gather = label1.unsqueeze(1)
        batch_indices = batch[2].cpu().numpy()

        embedding = embedding_layer(input_ids)
        embedding.requires_grad_(True)
        attention_mask = input_ids.ne(1)

        # ğŸ”¹ä¿®æ”¹ä½ç½® 1ï¼šè‡ªåŠ¨å¤„ç† logits â†’ æ­£ç±»æ¦‚ç‡
        logits_raw, _ = model(inputs_embeds=embedding, attention_mask=attention_mask)

        if model.config.num_labels == 1:
            # äºŒåˆ†ç±» (sigmoid)
            prob_of_correct_class = torch.sigmoid(logits_raw).squeeze(-1)
        else:
            # å¤šåˆ†ç±» (softmax)
            prob_softmax = torch.softmax(logits_raw, dim=-1)
            prob_of_correct_class = torch.gather(prob_softmax, 1, label_for_gather).squeeze()

        # ğŸ”¹ä¿®æ”¹ä½ç½® 2ï¼šä¿æŒ prob_diff é€»è¾‘ä¸€è‡´
        prob_diff = prob_of_correct_class - (1 - prob_of_correct_class)

        batch_token_grads = []
        for i in range(input_ids.size(0)):
            grad_i = torch.autograd.grad(
                outputs=prob_diff[i],
                inputs=embedding,
                retain_graph=True
            )[0][i]

            token_l2 = torch.norm(grad_i, p=2, dim=1)

            non_zero = token_l2 != 0
            valid_grad = token_l2[non_zero]

            normed_grad = torch.zeros_like(token_l2)
            if torch.numel(valid_grad) > 0 and valid_grad.max() > valid_grad.min():
                normed_grad[non_zero] = (valid_grad - valid_grad.min()) / (valid_grad.max() - valid_grad.min() + 1e-8)

            batch_token_grads.append(normed_grad.detach().cpu().numpy())

        # ä½¿ç”¨åŸå§‹idxä½œä¸ºé”®æ¥å¡«å……å­—å…¸
        for i, original_idx in enumerate(batch_indices):
            serializable_tokens = [str(t) for t in tokens_by_idx_for_lookup[original_idx]]
            tokens_by_idx_to_save[str(original_idx)] = serializable_tokens
            grads_by_idx_to_save[str(original_idx)] = batch_token_grads[i]

        logits.append(logits_raw.detach().cpu().numpy())
        labels.append(label.detach().cpu().numpy())

    # ä¿å­˜ tokens å’Œæ¢¯åº¦
    with open(os.path.join(args.output_dir, "tokens.json"), 'w', encoding='utf-8') as f:
        json.dump(tokens_by_idx_to_save, f, ensure_ascii=False, indent=2)
    np.savez(os.path.join(args.output_dir, "token_grad_norms.npz"), **grads_by_idx_to_save)

    return True



def predict_vulnerability(code, model, tokenizer, args):
    """
    å¯¹å•ä¸ªä»£ç ç‰‡æ®µè¿›è¡Œæ¼æ´é¢„æµ‹å¹¶æå–è¾“å…¥åµŒå…¥å±‚(input embeddings)çš„å€¼

    Args:
        code (str): è¦æ£€æµ‹çš„ä»£ç ç‰‡æ®µ
        model: å·²åŠ è½½çš„æ¨¡å‹
        tokenizer: å·²åŠ è½½çš„tokenizer
        args: å‚æ•°

    Returns:
        tuple: (æ˜¯å¦æœ‰æ¼æ´, æ¼æ´æ¦‚ç‡, è¾“å…¥åµŒå…¥, åµŒå…¥ä¿¡æ¯)
    """
    # æ„é€ è¾“å…¥ç‰¹å¾
    js = {"func": code, "idx": 0, "target": 0}  # æ ‡ç­¾æ— å…³ç´§è¦ï¼Œå› ä¸ºæˆ‘ä»¬åªå…³å¿ƒé¢„æµ‹ç»“æœ
    feature = codebert_convert_examples_to_features(js, tokenizer, args)

    # è½¬æ¢ä¸ºdataset
    dataset = []
    dataset.append((torch.tensor([feature.input_ids]), torch.tensor(feature.label), torch.tensor(feature.idx)))

    # è·å–åµŒå…¥å±‚
    active_path = 'encoder.roberta.embeddings.word_embeddings'
    try:
        parts = active_path.split('.')
        embedding_layer = model
        for part in parts:
            embedding_layer = getattr(embedding_layer, part)
        print(f"ä½¿ç”¨åµŒå…¥å±‚è·¯å¾„: {active_path}")
    except AttributeError:
        raise ValueError(f"æ— æ³•æ‰¾åˆ°åµŒå…¥å±‚è·¯å¾„ {active_path}ï¼Œè¯·æ£€æŸ¥æ¨¡å‹ç»“æ„")

    # é¢„æµ‹
    model.eval()

    inputs = torch.tensor([feature.input_ids]).to(args.device)

    # æå–è¾“å…¥åµŒå…¥å€¼
    # input_embeddings = embedding_layer(inputs).detach().cpu().numpy()
    input_embeddings = embedding_layer(inputs)

    # è·å–åµŒå…¥å±‚æƒé‡ä¿¡æ¯
    embedding_weights = embedding_layer.weight.detach().cpu().numpy()

    # è®¡ç®—åµŒå…¥ä¿¡æ¯
    embedding_info = {
        "active_path": active_path,
        "embedding_shape": input_embeddings.shape,
        "vocab_size": embedding_weights.shape[0],
        "embedding_dim": embedding_weights.shape[1],
        "tokens": [tokenizer.convert_ids_to_tokens(token_id) for token_id in feature.input_ids]
    }
    nums = 0
    all_nums = 0
    pad_nums = 0
    for token_id in feature.input_ids:
        all_nums += 1
        if tokenizer.convert_ids_to_tokens(token_id) != '<pad>':
            nums += 1
        else:
            pad_nums += 1
    # print(f"æ‰€æœ‰tokençš„æ•°é‡ï¼š{all_nums}")
    # print(f"épaddingæ•°é‡ï¼š{nums}")
    # print(f"paddingæ•°é‡ï¼š{pad_nums}\n")
    # print(f"åµŒå…¥å‘é‡å½¢çŠ¶: {input_embeddings.shape}")
    # print(f"è¯æ±‡è¡¨å¤§å°: {embedding_weights.shape[0]}")
    # print(f"åµŒå…¥ç»´åº¦: {embedding_weights.shape[1]}")

    outputs = model(input_ids = inputs, inputs_embeds = input_embeddings)
    # prob1 = outputs.cpu().numpy()[0][0]
    prob = outputs[0][0]

    is_vulnerable = prob > 0.5

    prob_diff = prob - (1 - prob)
    print(type(prob_diff))
    print(type(input_embeddings))

    embedding = torch.tensor(input_embeddings, dtype=torch.float32, requires_grad=True)

    emb_grad = torch.autograd.grad(
    outputs=prob_diff,
    inputs=input_embeddings,
    retain_graph=True,
    allow_unused=True
)[0]

    token_l2 = torch.norm(emb_grad, p=2, dim=2)  # å¯¹æ¯ä¸ª token çš„åµŒå…¥æ±‚èŒƒæ•°

    # å½’ä¸€åŒ– åªè€ƒè™‘éé›¶ä½ç½®çš„æœ€å¤§æœ€å°å½’ä¸€åŒ–
    non_zero = token_l2 != 0
    valid_grad = token_l2[non_zero]
    # æ‰§è¡Œå½’ä¸€åŒ–
    normed_grad = torch.zeros_like(token_l2)
    normed_grad[non_zero] = (valid_grad - valid_grad.min()) / (valid_grad.max() - valid_grad.min() + 1e-8)

    return is_vulnerable, prob, input_embeddings, embedding_info, emb_grad, token_l2, normed_grad


def vulnerability_detect(code, model, tokenizer, args):
    """
    å¯¹å•ä¸ªä»£ç ç‰‡æ®µè¿›è¡Œæ¼æ´æ£€æµ‹
    Args:
        code (str): è¦æ£€æµ‹çš„ä»£ç ç‰‡æ®µ
        model: å·²åŠ è½½çš„æ¨¡å‹
        tokenizer: å·²åŠ è½½çš„tokenizer
        args: å‚æ•°

    Returns:
        tuple: (æ˜¯å¦æœ‰æ¼æ´, æ¼æ´æ¦‚ç‡, è¾“å…¥åµŒå…¥, åµŒå…¥ä¿¡æ¯)
    """

    # æ„é€ è¾“å…¥ç‰¹å¾
    js = {"func": code, "idx": 0, "target": 0}  # æ ‡ç­¾æ— å…³ç´§è¦ï¼Œå› ä¸ºæˆ‘ä»¬åªå…³å¿ƒé¢„æµ‹ç»“æœ
    feature = codebert_convert_examples_to_features(js, tokenizer, args)

    # è½¬æ¢ä¸ºdataset
    dataset = []
    dataset.append((torch.tensor([feature.input_ids]), torch.tensor(feature.label), torch.tensor(feature.idx)))

    # é¢„æµ‹
    model.eval()

    inputs = torch.tensor([feature.input_ids]).to(args.device)

    prob, logit = model(input_ids=inputs)

    # logit = outputs[0][0]
    #
    # prob = torch.sigmoid(logit)

    is_vulnerable = logit > 0

    prob_diff = prob - (1 - prob)


    return is_vulnerable, logit, prob